# -*- coding: utf-8 -*-
"""Copy [college]of HousePricePrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hzw8pqUxbLN7vmegE45h5c0ggXslNo60

# *House Price Prediction *

*Importing necessary libraries and loading data*
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df=pd.read_csv('/content/housing.csv')

"""*Data Pre-processing*"""

df.head()

df.info()

print(df.isnull().sum())

df.dropna()

df1=df.dropna()

df1.info()

df1.describe()

"""*Exploratory Data Analysis*

"""

# Histograms of numerical features

df.hist(bins =50,figsize=(12, 10))
plt.suptitle('Histograms of Numerical Features')
plt.show()

"""* The distribution of median income is right-skewed, indicating that most houses
have a relatively low median income.
* The distribution of housing age is roughly normal, with a slight skew towards older houses.
* The distribution of the number of bedrooms is roughly normal, with a slight skew towards houses with fewer bedrooms.
* The distribution of the number of bathrooms is roughly normal, with a slight skew towards houses with fewer bathrooms.
* The distribution of the number of rooms is roughly normal, with a slight skew towards houses with fewer rooms.
* The distribution of the population is roughly normal, with a slight skew towards areas with fewer people.
* The distribution of the number of households is roughly normal, with a slight skew towards areas with fewer households.
* The distribution of median house value is right-skewed, indicating that most houses have a relatively low median value.
"""

#scatter plot of 'median_income' vs 'median_house_value'

plt.figure(figsize=(8, 6))
plt.scatter(df['median_income'], df['median_house_value'], alpha=0.5)
plt.title('Scatter Plot of Median Income vs Median House Value')
plt.xlabel('Median Income')
plt.ylabel('Median House Value')
plt.show()

"""
 * The scatter plot shows a positive correlation between median income and median house value.
 * This means that as median income increases, median house value also tends to increase.
 * The correlation is not perfect, however, as there are some houses with high median incomes that have low median house values, and vice versa.
 * This suggests that there are other factors besides median income that influence median house value."""

# Bar plot of 'ocean_proximity'

plt.figure(figsize=(8, 6))
df1['ocean_proximity'].value_counts().plot(kind='bar')
plt.title('Bar Plot of Ocean Proximity')
plt.xlabel('Ocean Proximity')
plt.ylabel('Frequency')
plt.show()

""" * The median house value is highest for houses located near the ocean.
 * The median house value is lowest for houses located near the bay.
 * The interquartile range (IQR) is widest for houses located near the ocean, indicating a greater spread in house values.
 * The IQR is narrowest for houses located near the bay, indicating a smaller spread in house values.
 * There are more outliers for houses located near the ocean, indicating that there are some houses with very high or very low values.
 * There are fewer outliers for houses located near the bay, indicating that there are fewer houses with very high or very low values.
"""

# Geographical scatter plot using latitude and longitude


plt.figure(figsize=(10, 8))
plt.scatter(df1['longitude'], df1['latitude'], alpha=0.4, c=df1['median_house_value'], cmap='viridis')
plt.colorbar(label='Median House Value')
plt.title('Geographical Scatter Plot of Housing Prices')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

"""
* The median house value is highest in areas with a longitude between -122.5 and -122.0 and a latitude between 37.7 and 38.0.
* The median house value is lowest in areas with a longitude between -124.0 and -123.5 and a latitude between 37.0 and 37.5.
* There is a general trend of decreasing median house value as one moves away from the coast.
* This suggests that proximity to the coast is a major factor in determining housing prices in this region.
* There are some areas with high median house values that are located inland.
* This suggests that there are other factors besides proximity to the coast that can influence housing prices."""

import seaborn as sns

# COrrealtional heatmap for numerical features

# Exclude non-numeric columns
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

# Compute the correlation matrix for numeric columns
corr_matrix = df[numeric_cols].corr()

# Plot the heatmap
plt.figure(figsize=(15, 10))
sns.heatmap(corr_matrix, annot=True, cmap="YlGnBu")
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

""" The correlation heatmap shows the strength and direction of the linear relationship between each pair of numerical features.
 The values in the heatmap range from -1 to 1, where:
 * -1 indicates a perfect negative correlation.
 * 0 indicates no correlation.
 * 1 indicates a perfect positive correlation.
 The following are some of the key inferences that can be drawn from the heatmap:
 * Median house value has a strong positive correlation with median income, indicating that houses in areas with higher median incomes tend to have higher median house values.
 * Median house value has a strong positive correlation with the number of rooms, indicating that houses with more rooms tend to have higher median house values.
 * Median house value has a strong positive correlation with the number of bedrooms, indicating that houses with more bedrooms tend to have higher median house values.
 * Median house value has a strong positive correlation with the number of bathrooms, indicating that houses with more bathrooms tend to have higher median house values.
 * Median house value has a strong positive correlation with population, indicating that houses in areas with higher populations tend to have higher median house values.
 * Median house value has a strong positive correlation with the number of households, indicating that houses in areas with more households tend to have higher median house values.
 * Median house value has a strong negative correlation with housing age, indicating that older houses tend to have lower median house values.
 * Median house value has a strong positive correlation with latitude, indicating that houses located further north tend to have higher median house values.
 * Median house value has a strong negative correlation with longitude, indicating that houses located further west tend to have lower median house values.
"""

## since there are 4 features which are skewed (right skewed ) we will take logarithm transformation to reduce the skewness.

import numpy as np

# List of skewed features
skewed_features = ['total_rooms', 'total_bedrooms', 'population', 'households']

# Apply logarithm transformation to skewed features
for feature in skewed_features:
    df1[feature] = np.log1p(df1[feature])

# Visualize histograms of transformed features to check for skewness reduction
df1[skewed_features].hist(bins=50,figsize=(12, 10))
plt.suptitle('Histograms of Log-Transformed Skewed Features')
plt.show()

df1.ocean_proximity.value_counts()

#hot encoding to convert the ocean proximity unique values into 4 different columns int he form of 1 and 0s

df1 = df1.join(pd.get_dummies(df['ocean_proximity'], prefix='ocean').astype(int))
df1 = df1.drop('ocean_proximity', axis=1)

df1

# COrrealtional heatmap for numerical features

# Exclude non-numeric columns
numeric_cols = df1.select_dtypes(include=['float64', 'int64']).columns

# Compute the correlation matrix for numeric columns
corr_matrix = df1[numeric_cols].corr()

# Plot the heatmap
plt.figure(figsize=(15, 10))
sns.heatmap(corr_matrix, annot=True, cmap="YlGnBu")
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

"""This is the plot after hot encoding

* The correlation matrix shows the strength and direction of the linear relationship between each pair of numerical features just as the above one.
* The difference is only of the features which are splitted in the basis of their unique values of a particular column
"""

plt.figure(figsize=(15,8))
sns.scatterplot(x="latitude", y="longitude", data=df1,hue="median_house_value", palette="coolwarm")

"""This is the scatterplot after hot encoding which has the similiar inferences as that of the previous plot"""





























"""#Feature Engineering"""

# from sklearn.linear_model import LinearRegression

# # Separate features and target variable
# X_train = df1.drop(['median_house_value'], axis=1)
# y_train = df1['median_house_value']

# # Initialize and fit linear regression model
# reg = LinearRegression()
# reg.fit(X_train, y_train)

from sklearn.linear_model import LinearRegression
# Separate features and target variable
X_train = df1.drop(['median_house_value'], axis=1)
y_train = df1['median_house_value']
# Initialize and fit linear regression model
reg = LinearRegression()
reg.fit(X_train, y_train)

train_data = X_train.join(y_train)

train_data

train_data['bedroom_ratio'] = train_data['total_bedrooms'] / train_data['total_rooms']
train_data['household_rooms'] = train_data['total_rooms'] / train_data['households']

from sklearn.linear_model import LinearRegression

# Perform one-hot encoding for categorical variables
X_train = pd.get_dummies(train_data.drop(['median_house_value'], axis=1))

# Separate features and target variable
y_train = train_data['median_house_value']

# Initialize and fit linear regression model
reg = LinearRegression()
reg.fit(X_train, y_train)

df1.head(10)

df1.columns

from sklearn.model_selection import train_test_split

# Define X (features) and y (target variable)
X = df1.drop(['median_house_value'], axis=1)
y = df1['median_house_value']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.impute import SimpleImputer

# Initialize the imputer
imputer = SimpleImputer(strategy='mean')

# Fit and transform the training data
X_train_imputed = imputer.fit_transform(X_train)

# Transform the testing data (only transform, don't fit again)
X_test_imputed = imputer.transform(X_test)

# Initialize and fit linear regression model with imputed data
reg_imputed = LinearRegression()
reg_imputed.fit(X_train_imputed, y_train)

# Evaluate the model
score_imputed = reg_imputed.score(X_test_imputed, y_test)
print("Model Score with Imputed Data:", score_imputed)

print(X_test)

# prompt: print models performance metrices

from sklearn.metrics import mean_squared_error, r2_score

# Predict using the model
y_pred = reg_imputed.predict(X_test_imputed)

# Calculate performance metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the metrics
print("Mean Squared Error:", mse)
print("R-squared Score:", r2)

"""# Random forest -model"""

from sklearn.ensemble import RandomForestRegressor

forest = RandomForestRegressor()

forest.fit(X_train, y_train)

from sklearn.impute import SimpleImputer

# Initialize the imputer
imputer = SimpleImputer(strategy='mean')

# Fit the imputer on the training data
imputer.fit(X_train)

# Transform both the training and test data
X_train_imputed = imputer.transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Now you can use the imputed data to train and evaluate your model

"""# Hyper-tuning the model"""

from sklearn.model_selection import GridSearchCV

forest = RandomForestRegressor()

param_grid = {
    "n_estimators": [3, 10 ,30],
    "max_features": [2,4, 6 ,8]
}

grid_search = GridSearchCV(forest, param_grid, cv=5,
                           scoring="neg_mean_squared_error",
                           return_train_score=True)

grid_search.fit(X_train, y_train)

grid_search.best_estimator_

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error

# Initialize the RandomForestRegressor with adjusted hyperparameters
forest = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=42)

# Fit the RandomForestRegressor to the training data
forest.fit(X_train_imputed, y_train)

# Make predictions on the training data
y_train_pred = forest.predict(X_train_imputed)

# Make predictions on the test data
y_pred = forest.predict(X_test_imputed)

# Calculate R-squared score and mean squared error for training data
r2_train = r2_score(y_train, y_train_pred)
mse_train = mean_squared_error(y_train, y_train_pred)

# Calculate R-squared score and mean squared error for testing data
r2_test = r2_score(y_test, y_pred)
mse_test = mean_squared_error(y_test, y_pred)

# Print the model's performance metrics
print("Training R-squared score:", r2_train)
print("Training Mean Squared Error:", mse_train)
print("Testing R-squared score:", r2_test)
print("Testing Mean Squared Error:", mse_test)

# prompt: how to find out the version of skitlearn

import sklearn
print(sklearn.__version__)

# Save the trained model
import joblib
joblib.dump(forest, 'forest.pkl')

